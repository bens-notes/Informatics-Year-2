{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian distribution is a reasonable model in many situations, thus is very usefull for machine learning.\n",
    "\n",
    "$$\n",
    "p(x \\mid \\mu, \\sigma^2) = N(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( \\frac{-(x - \\mu)^2}{2 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "The parameters are:\n",
    "- The mean $\\mu$ (location)\n",
    "- The variance $\\sigma^2$ (dispersion)\n",
    "\n",
    "A gaussian can be used to describe a random variable that cluster around some mean value\n",
    "\n",
    "- 68% of values are within one standard deviation ($\\pm \\sigma)$ from the mean\n",
    "- 95% of values are within two standard deviation ($\\pm 2\\sigma)$ from the mean\n",
    "- 99.7% of values are within three standard deviation ($\\pm 3\\sigma)$ from the mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelhood estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset $\\left\\{ x_1, x_2, ..., x_N \\right\\}$ we can get unbiased estimates of the sample mean and sample variance with:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mu} = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat{\\sigma}^2 = \\frac{1}{N - 1} \\sum_{n=1}^N (x_n - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "The maximum likelihood estimates are simular:\n",
    "\n",
    "$$\n",
    "\\widehat{\\mu}_{ML} = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\widehat{\\sigma}^2_{ML} = \\frac{1}{N} \\sum_{n=1}^N (x_n - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "To show were the maximum likehood estimates come from, we will consider the optermization problem for a set $\\mathcal{D}$ training samples.\n",
    "\n",
    "$$\n",
    "\\max_{\\mu, \\sigma} p(\\mathcal{D} \\mid \\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "If we make the assumption that the samples in $\\mathcal{D}$ are independently drawn from the same distribution, we get $L$, the likehood function:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(\\mathcal{D} \\mid \\mu, \\sigma^2) &= p(x_1, ..., x_N \\mid \\mu, \\sigma^2) \\\\\n",
    "&= p(x_1 \\mid \\mu, \\sigma^2) ... p(x_N \\mid \\mu, \\sigma^2) \\\\\n",
    "&= \\prod_{n=1}^N p(x_n \\mid mu, \\sigma^2) \\\\\n",
    "&= L(\\mu, \\sigma^2 \\mid \\mathcal{D})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Taking the log of $L$ gives us the log-likehood function $LL$ (this doesnt change are optimization problem since log is a monotonic increasing function)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "LL(\\mu, \\sigma^2 \\mid \\mathcal{D}) &= \\ln L(\\mu, \\sigma \\mid \\mathcal{D}) \\\\\n",
    "&= \\ln \\prod_{n=1}^N p(x_n \\mid \\mu, \\sigma^2) \\\\\n",
    "&= \\sum_{n=1}^N \\ln p(x_n \\mid \\mu, \\sigma^2) \\\\\n",
    "&= \\sum_{n=1}^N \\ln \\left( \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left( \\frac{-(x_n - \\mu)^2}{2 \\sigma^2} \\right) \\right) \\\\\n",
    "&= - \\frac{N}{2} \\ln (2 \\pi) - \\frac{N}{2} \\ln (\\sigma^2) - \\sum_{n=1}^N \\frac{(x_n - \\mu)^2}{2 \\sigma^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now to maximize, we will take the partial derivative with respect to each parameter and set it equal to zero\n",
    "\n",
    "$$\n",
    "\\frac{\\partial LL(\\mu, \\sigma^2 \\mid \\mathcal{D})}{\\partial \\mu} = 0 \\implies \\hat{\\mu} = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial LL(\\mu, \\sigma^2 \\mid \\mathcal{D})}{\\partial \\sigma} = 0 \\implies \\hat{\\mu} = \\frac{1}{N} \\sum_{n=1}^N (x_n - \\hat{\\mu})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending to $D$-dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A $D$ dimensional vector $\\mathbf{x} = (x_1, ..., x_D)^T is _multivariate Gaussian_ if it has a pdf in the form:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{\\mu, \\Sigma}) = \\frac{1}{(2\\pi)^{d/2} |\\mathbf{\\Sigma}|^{1/2}} \\exp \\left( - \\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right)\n",
    "$$\n",
    "\n",
    "$\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\mathbf{\\Sigma}^{-1} (\\mathbf{x} - \\mathbf{\\mu}$ is referred to as a _quadratic form_.\n",
    "\n",
    "The parameters are:\n",
    "\n",
    "- $\\mathbf{\\mu}$ the mean vector, $\\mathbf{\\mu} = E(X)$\n",
    "- $\\mathbf{\\Sigma}$ the covariance matrix, $\\mathbf{\\Sigma} = E((\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^T)$\n",
    "\n",
    "The covariance matrix is a $D \\times D$ symetric matrix, that is $\\mathbf{\\Sigma}^T = \\mathbf{\\Sigma}$. The elements of $\\Sigma$ show the relation ship of two components:\n",
    "\n",
    "- If $x_j$ is large when $x_i$ is large, then $(x_j - \\mu_j)(x_i - \\mu_i)$ will tend to be positive\n",
    "- If $x_j$ is small when $x_i$ is large, then $(x_j - \\mu_j)(x_i - \\mu_i)$ will tend to be negative\n",
    "\n",
    "The variance of $x_i$ is $\\sigma_i^2$ which is equivalent to the covariance between $x_i$ and $x_i$, that is $\\sigma_i^2 = \\sigma_{ii}$.\n",
    "\n",
    "The unbiased and maximum likelihood estimations become:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{\\mu} &= E(\\mathbf{x})\\\\\n",
    "\\hat{\\mathbf{\\mu}}_{ML} &= \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n \\\\\n",
    "\\mathbf{\\Sigma} &= E((\\mathbf{x} - \\mathbf{\\mu})(\\mathbf{x} - \\mathbf{\\mu})^T) \\\\\n",
    "\\hat{\\mathbf{\\Sigma}}_{ML} &= \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{x}_n - \\hat{\\mathbf{\\mu}}_{ML})(\\mathbf{x}_n - \\hat{\\mathbf{\\mu}}_{ML})^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The covariance matrix is not scale independent, to fix this we introduce the _correlation matix_ $R$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "R &= (\\rho_{ij}) \\\\\n",
    "\\rho_{ij} &= \\frac{\\sigma_{ij}}{\\sqrt{\\sigma_{ii} \\sigma_{jj}}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This new terms are now scale and location independent, that is:\n",
    "\n",
    "$$\n",
    "\\rho(x_i, x_j) = \\rho(a x_i +b, c x_j + d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-D Gaussian examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](res/gaussian_example_1.png)\n",
    "![](res/gaussian_example_2.png)\n",
    "![](res/gaussian_example_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Calculate $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$ for the samples $\\displaystyle\\mathbf{x} : \\binom{5}{1}, \\binom{5}{2}, \\binom{7}{2}, \\binom{7}{3}$\n",
    "\n",
    "First, using $\\mathbf{\\mu} = \\frac{1}{N} \\sum_{n=1}^N \\mathbf{x}_n$ we will compute $\\mathbf{\\mu}$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{\\mu} \\\\\n",
    "&= \\frac{1}{4} \\left\\{ \\begin{bmatrix}5\\\\1\\end{bmatrix} + \\begin{bmatrix}5\\\\2\\end{bmatrix} + \\begin{bmatrix}7\\\\2\\end{bmatrix} + \\begin{bmatrix}7\\\\3\\end{bmatrix} \\right\\} \\\\\n",
    "&= \\begin{bmatrix}6\\\\2\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "To compute $\\mathbf{\\Sigma}$ we will first compute $\\mathbf{x}_n - \\mathbf{\\mu}$\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_n - \\mathbf{\\mu} : \\binom{-1}{-1}, \\binom{-1}{0}, \\binom{1}{0}, \\binom{1}{1}\n",
    "$$\n",
    "\n",
    "Finnally using $\\mathbf{\\Sigma} = \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{x}_n - \\hat{\\mathbf{\\mu}}_{ML})(\\mathbf{x}_n - \\hat{\\mathbf{\\mu}}_{ML})^T$ \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{\\Sigma} &= \\frac{1}{4} \\left\\{ \n",
    "\\begin{bmatrix}-1\\\\-1\\end{bmatrix} \\begin{bmatrix}-1 & -1\\end{bmatrix}+\n",
    "\\begin{bmatrix}-1\\\\0\\end{bmatrix} \\begin{bmatrix}-1 & 0\\end{bmatrix}+\n",
    "\\begin{bmatrix}1\\\\0\\end{bmatrix} \\begin{bmatrix}1 & 0\\end{bmatrix}+\n",
    "\\begin{bmatrix}1\\\\1\\end{bmatrix} \\begin{bmatrix}1&1\\end{bmatrix}\n",
    "\\right\\} \\\\\n",
    "&= \\begin{bmatrix}1&\\frac{1}{2}\\\\\\frac{1}{2}&\\frac{1}{2}\\end{bmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Contour plot:\n",
    "\n",
    "![](res/example_contour.png)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
