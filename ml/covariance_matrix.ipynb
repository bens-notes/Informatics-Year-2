{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing covariance matrix calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance matrix written in this form\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{x}_n - \\mathbf{\\mu})(\\mathbf{x}_n - \\mathbf{\\mu})^T\n",
    "$$\n",
    "\n",
    "would requires a for loop to iterate over each sample. To avoid this we can replace the sumation with matrix operations\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma &= \\frac{1}{N} \\sum_{n=1}^N (\\mathbf{x}_n - \\mathbf{\\mu})(\\mathbf{x}_n - \\mathbf{\\mu})^T \\\\\n",
    "&= \\frac{1}{N} (\\mathbf{x}_1 - \\mathbf{\\mu}, ..., \\mathbf{x}_N - \\mathbf{\\mu}) \\begin{pmatrix}\n",
    "\\mathbf{x}_1^T - \\mathbf{\\mu}^T \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^T - \\mathbf{\\mu}^T \\\\\n",
    "\\end{pmatrix} \\\\\n",
    "&= \\frac{1}{N} (X- M_N)^T (X - M_N)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "X = \\begin{bmatrix}\n",
    "x_{11} & \\cdots & x_{1D} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "x_{N1} & \\cdots & x_{ND}\n",
    "\\end{bmatrix}\n",
    "\\quad\\quad\n",
    "M_N &= \\begin{bmatrix}\n",
    "\\mu_1 & \\cdots & \\mu_D \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "\\mu_1 & \\cdots & \\mu_D\n",
    "\\end{bmatrix}\\\\\n",
    "&= \\frac{1}{N} 1_{NN} X\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Where $1_{NN}$ is a $N \\times N$ matrix were all the elements are $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More properties of covariance matrix ($\\dagger$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split any covariance matrix into three parts\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Sigma &= V D V^T \\\\\n",
    "&= \\begin{pmatrix}\n",
    "v_{11} & \\cdots & v_{1D} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "v_{N1} & \\cdots & v_{ND}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\lambda_1 &  & 0 \\\\\n",
    " & \\ddots &  \\\\\n",
    "0 & & \\lambda_D\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{11} & \\cdots & v_{1D} \\\\\n",
    "\\vdots & \\ddots & \\vdots \\\\\n",
    "v_{D1} & \\cdots & v_{DD}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- All eigenvalues in the diagonal matrix will be non-negative\n",
    "- The determinent $|\\Sigma|$ is equal to $\\prod_{i=1}^D \\lambda_i$\n",
    "- The sum of variances $\\sum_{i=1}^D \\sigma_{ii}$ is equal to the sum of eigenvalues $\\sum_{i=1}^D \\lambda_i$\n",
    "\n",
    "From the rank of $\\Sigma$ we can obtain the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "rank(\\Sigma) = D \\rightarrow & \\forall_i : \\lambda_i > 0 \\\\\n",
    "& \\forall_{i \\ne j} : \\mathbf{v}_i \\bot \\mathbf{v}_j \\\\\n",
    "& | \\Sigma | > 0\\\\\n",
    "rank(\\Sigma) < D \\rightarrow & \\exists_i : \\lambda_i = 0 \\\\\n",
    "& \\exists_{(i,j)} : \\rho(x_i, x_j) \\\\\n",
    "& | \\Sigma | = 0\\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with estimation of covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$|\\Sigma|$ is almost (or equal to) zero when\n",
    "\n",
    "- $N$ is not large enough (compared to $D$) $|\\Sigma| = 0$ for $N \\leq D$\n",
    "- There is high dependence (correlation) between variables ($\\rho(x_i, x_j) \\approx 1$)\n",
    "\n",
    "Due to the nature of floating point numbers, the inverse of $\\Sigma^{-1}$ when $|\\Sigma|$ is small is unstable. Some solutions are:\n",
    "\n",
    "- Share $\\Sigma$ among classes\n",
    "- Assume independence amoung variables (naive bayes assumption), this yields a diagonal covariance matrix rather than a full covariance matrix\n",
    "- Reduce the dimensionality of the data (e.g. PCA)\n",
    "- Add a small positive constant $\\epsilon$ to the diagonal elements, $\\Sigma + \\epsilon I$\n",
    "\n",
    "To share $\\Sigma$ among classes $1, ..., K$ we take the average of each covariance matrix\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{K} \\sum_{k=1}^K \\Sigma_k\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
